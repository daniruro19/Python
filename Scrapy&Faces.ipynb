{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniruro19/Python/blob/main/Scrapy%26Faces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del proyecto"
      ],
      "metadata": {
        "id": "pf52ZoDEgHjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-JjL1SQFgDG5",
        "outputId": "d06b7dee-f2f5-411c-9583-46d52f5a1b4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Scrapy\n",
            "  Downloading Scrapy-2.7.1-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from Scrapy) (57.4.0)\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from Scrapy) (4.9.1)\n",
            "Collecting pyOpenSSL>=21.0.0\n",
            "  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting service-identity>=18.1.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting Twisted>=18.9.0\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 49.0 MB/s \n",
            "\u001b[?25hCollecting zope.interface>=5.1.0\n",
            "  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from Scrapy) (21.3)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n",
            "Collecting cryptography>=3.3\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 37.6 MB/s \n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=3.3->Scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=3.3->Scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from protego>=0.1.15->Scrapy) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->Scrapy) (22.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->Scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->Scrapy) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->Scrapy) (4.4.0)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.8/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->Scrapy) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->Scrapy) (3.0.9)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from tldextract->Scrapy) (3.8.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from tldextract->Scrapy) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->Scrapy) (2022.9.24)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11959 sha256=00202730902b9123736dff00fca348a7c3edeead6d5e85f365d48be1014a89d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/b9/4a/948b1176e084b9e3f85e4ffc3d08f817b1fdf0d973bbb94f81\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, Scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Scrapy-2.7.1 Twisted-22.10.0 constantly-15.1.0 cryptography-38.0.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.7.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.1.1 zope.interface-5.5.2\n"
          ]
        }
      ],
      "source": [
        "# instalación de Scrapy\n",
        "!pip install Scrapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creación del proyecto\n",
        "!scrapy startproject project_faces scrapy"
      ],
      "metadata": {
        "id": "CbPN2xf_gU5l",
        "outputId": "2fed3088-4384-44db-e849-d191697e9a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'project_faces', using template directory '/usr/local/lib/python3.8/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/scrapy\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd scrapy\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creación del spider\n",
        "!cd scrapy/project_faces && scrapy genspider FindFaces https://www.tinosoriano.com"
      ],
      "metadata": {
        "id": "ej1L2-6Bgnzq",
        "outputId": "b48d37fa-71ad-4076-e4fa-71bd5c1d45dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'FindFaces' using template 'basic' in module:\n",
            "  project_faces.spiders.FindFaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edición de path/to/settings.py\n",
        "\n",
        "\n",
        "\n",
        "*   USER_AGENT\n",
        "*   ROBOTSTXT_OBEY\n",
        "*   DEFAULT_REQUEST_HEADERS\n",
        "\n"
      ],
      "metadata": {
        "id": "NCJ62iCGibB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scrapy/project_faces/settings.py\n",
        "\n",
        "BOT_NAME = 'project_faces'\n",
        "\n",
        "SPIDER_MODULES = ['project_faces.spiders']\n",
        "NEWSPIDER_MODULE = 'project_faces.spiders'\n",
        "\n",
        "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
        "\n",
        "ROBOTSTXT_OBEY = False\n",
        "\n",
        "DEFAULT_REQUEST_HEADERS = {\n",
        "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "   'Accept-Language': 'es',\n",
        "}\n",
        "\n",
        "REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.7'\n",
        "TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'"
      ],
      "metadata": {
        "id": "7In0vQc-lJD_",
        "outputId": "cfacb2c4-ccc6-460f-dad5-d60328cad925",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scrapy/project_faces/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecución del spider\n",
        "\n",
        "### No se debe ejecutar hasta que se haya implementado el spider."
      ],
      "metadata": {
        "id": "sJchdLIejozv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd scrapy/project_faces && scrapy crawl FindFaces"
      ],
      "metadata": {
        "id": "aXCHJ1RFBXsg",
        "outputId": "f8461261-d2ea-40c3-8852-62669034099f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/cmdline.py\", line 153, in execute\n",
            "    cmd.crawler_process = CrawlerProcess(settings)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/crawler.py\", line 304, in __init__\n",
            "    super().__init__(settings)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/crawler.py\", line 181, in __init__\n",
            "    self.spider_loader = self._get_spider_loader(settings)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/crawler.py\", line 175, in _get_spider_loader\n",
            "    return loader_cls.from_settings(settings.frozencopy())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/spiderloader.py\", line 67, in from_settings\n",
            "    return cls(settings)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/spiderloader.py\", line 24, in __init__\n",
            "    self._load_all_spiders()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/spiderloader.py\", line 51, in _load_all_spiders\n",
            "    for module in walk_modules(name):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/scrapy/utils/misc.py\", line 89, in walk_modules\n",
            "    submod = import_module(fullpath)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/scrapy/project_faces/spiders/FindFaces.py\", line 3\n",
            "    %%writefile scrapy/project_faces/spiders/FindFaces.py\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd scrapy/project_faces && scrapy crawl FindFaces -o faces.json"
      ],
      "metadata": {
        "id": "OcOEA-_0j35Z",
        "outputId": "61a047d0-4a2e-4f6b-cb71-fd93cdf5fbfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-15 16:44:42 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: project_faces)\n",
            "2022-12-15 16:44:42 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.16 (default, Dec  7 2022, 01:12:13) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.4, Platform Linux-5.10.133+-x86_64-with-glibc2.27\n",
            "2022-12-15 16:44:42 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'project_faces',\n",
            " 'NEWSPIDER_MODULE': 'project_faces.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'SPIDER_MODULES': ['project_faces.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'}\n",
            "2022-12-15 16:44:42 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2022-12-15 16:44:42 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2022-12-15 16:44:42 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2022-12-15 16:44:42 [scrapy.extensions.telnet] INFO: Telnet Password: c4c1120828ab509b\n",
            "2022-12-15 16:44:42 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-12-15 16:44:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-12-15 16:44:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-12-15 16:44:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-12-15 16:44:42 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-12-15 16:44:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-12-15 16:44:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-12-15 16:44:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/> (referer: None)\n",
            "numero de entradas: 3\n",
            "href https://www.investigart.com/2022/12/13/la-capilla-sixtina-del-barroco-frances/\n",
            "href https://www.investigart.com/2022/12/07/orgullobarroco-en-un-lugar-de-la-macha-herencia/\n",
            "href https://www.investigart.com/2022/11/29/el-descendimiento-de-bronzino-dos-obras-para-un-mismo-comitente/\n",
            "2022-12-15 16:44:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/page/2/?et_blog> (referer: https://www.investigart.com/)\n",
            "numero de entradas: 3\n",
            "href https://www.investigart.com/2022/11/22/emulando-a-caravaggio-louis-finson-entre-michelangelo-merisi-y-la-tradicion-flamenca/\n",
            "href https://www.investigart.com/2022/11/15/el-apostolado-de-georges-de-la-tour-en-albi/\n",
            "href https://www.investigart.com/2022/11/01/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos-2/\n",
            "2022-12-15 16:44:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/12/13/la-capilla-sixtina-del-barroco-frances/> (referer: https://www.investigart.com/)\n",
            "2022-12-15 16:44:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/12/07/orgullobarroco-en-un-lugar-de-la-macha-herencia/> (referer: https://www.investigart.com/)\n",
            "2022-12-15 16:44:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/12/13/la-capilla-sixtina-del-barroco-frances/>\n",
            "{'href': 'https://www.investigart.com/2022/12/13/la-capilla-sixtina-del-barroco-frances/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/12/Vista-general-scaled.jpg'}\n",
            "2022-12-15 16:44:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/12/07/orgullobarroco-en-un-lugar-de-la-macha-herencia/>\n",
            "{'href': 'https://www.investigart.com/2022/12/07/orgullobarroco-en-un-lugar-de-la-macha-herencia/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/12/318407191_512829410878356_2455083222164072723_n.jpg'}\n",
            "2022-12-15 16:44:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/11/29/el-descendimiento-de-bronzino-dos-obras-para-un-mismo-comitente/> (referer: https://www.investigart.com/)\n",
            "2022-12-15 16:44:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/11/29/el-descendimiento-de-bronzino-dos-obras-para-un-mismo-comitente/>\n",
            "{'href': 'https://www.investigart.com/2022/11/29/el-descendimiento-de-bronzino-dos-obras-para-un-mismo-comitente/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/11/Capilla-Leonor.jpg'}\n",
            "2022-12-15 16:44:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/page/3/?et_blog> (referer: https://www.investigart.com/page/2/?et_blog)\n",
            "numero de entradas: 3\n",
            "href https://www.investigart.com/2022/10/25/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos/\n",
            "href https://www.investigart.com/2022/10/18/mariana-de-neoburgo-ultima-reina-de-los-austrias-vida-y-legado-artistico/\n",
            "href https://www.investigart.com/2022/10/11/lost-in-translation-de-la-importancia-de-la-investigacion-y-la-reelectura-documental/\n",
            "2022-12-15 16:44:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/11/01/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos-2/> (referer: https://www.investigart.com/page/2/?et_blog)\n",
            "2022-12-15 16:44:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/11/22/emulando-a-caravaggio-louis-finson-entre-michelangelo-merisi-y-la-tradicion-flamenca/> (referer: https://www.investigart.com/page/2/?et_blog)\n",
            "2022-12-15 16:44:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/11/01/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos-2/>\n",
            "{'href': 'https://www.investigart.com/2022/11/01/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos-2/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/11/MC122.jpg'}\n",
            "2022-12-15 16:44:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/11/22/emulando-a-caravaggio-louis-finson-entre-michelangelo-merisi-y-la-tradicion-flamenca/>\n",
            "{'href': 'https://www.investigart.com/2022/11/22/emulando-a-caravaggio-louis-finson-entre-michelangelo-merisi-y-la-tradicion-flamenca/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/11/IMG_20220802_174818.jpg'}\n",
            "2022-12-15 16:44:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/10/18/mariana-de-neoburgo-ultima-reina-de-los-austrias-vida-y-legado-artistico/> (referer: https://www.investigart.com/page/3/?et_blog)\n",
            "2022-12-15 16:44:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/11/15/el-apostolado-de-georges-de-la-tour-en-albi/> (referer: https://www.investigart.com/page/2/?et_blog)\n",
            "2022-12-15 16:44:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/10/25/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos/> (referer: https://www.investigart.com/page/3/?et_blog)\n",
            "2022-12-15 16:44:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/10/18/mariana-de-neoburgo-ultima-reina-de-los-austrias-vida-y-legado-artistico/>\n",
            "{'href': 'https://www.investigart.com/2022/10/18/mariana-de-neoburgo-ultima-reina-de-los-austrias-vida-y-legado-artistico/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/10/mariana_de_neoburgo_portada-scaled.jpg'}\n",
            "2022-12-15 16:44:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/11/15/el-apostolado-de-georges-de-la-tour-en-albi/>\n",
            "{'href': 'https://www.investigart.com/2022/11/15/el-apostolado-de-georges-de-la-tour-en-albi/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/11/Santiago-el-Menor-scaled.jpg'}\n",
            "2022-12-15 16:44:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/10/25/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos/>\n",
            "{'href': 'https://www.investigart.com/2022/10/25/ora-et-labora-el-papel-de-las-mujeres-artistas-en-los-conventos/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/10/Captura-de-pantalla-92.png'}\n",
            "2022-12-15 16:44:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.investigart.com/2022/10/11/lost-in-translation-de-la-importancia-de-la-investigacion-y-la-reelectura-documental/> (referer: https://www.investigart.com/page/3/?et_blog)\n",
            "2022-12-15 16:44:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.investigart.com/2022/10/11/lost-in-translation-de-la-importancia-de-la-investigacion-y-la-reelectura-documental/>\n",
            "{'href': 'https://www.investigart.com/2022/10/11/lost-in-translation-de-la-importancia-de-la-investigacion-y-la-reelectura-documental/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/09/Ugolino_Martelli_Angelo_Bronzino_-_GemÑldegalerie_Berlin_9478.jpg'}\n",
            "2022-12-15 16:44:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-12-15 16:44:50 [scrapy.extensions.feedexport] INFO: Stored json feed (9 items) in: faces.json\n",
            "2022-12-15 16:44:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 4783,\n",
            " 'downloader/request_count': 12,\n",
            " 'downloader/request_method_count/GET': 12,\n",
            " 'downloader/response_bytes': 1023550,\n",
            " 'downloader/response_count': 12,\n",
            " 'downloader/response_status_count/200': 12,\n",
            " 'elapsed_time_seconds': 7.578357,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 12, 15, 16, 44, 50, 559555),\n",
            " 'httpcompression/response_bytes': 4380904,\n",
            " 'httpcompression/response_count': 12,\n",
            " 'item_scraped_count': 9,\n",
            " 'log_count/DEBUG': 24,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 123674624,\n",
            " 'memusage/startup': 123674624,\n",
            " 'request_depth_max': 3,\n",
            " 'response_received_count': 12,\n",
            " 'scheduler/dequeued': 12,\n",
            " 'scheduler/dequeued/memory': 12,\n",
            " 'scheduler/enqueued': 12,\n",
            " 'scheduler/enqueued/memory': 12,\n",
            " 'start_time': datetime.datetime(2022, 12, 15, 16, 44, 42, 981198)}\n",
            "2022-12-15 16:44:50 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación del spider"
      ],
      "metadata": {
        "id": "Cq1PRakRkNaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión inicial"
      ],
      "metadata": {
        "id": "xfymL2gXkWGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión modificada"
      ],
      "metadata": {
        "id": "8E0AtX1ulM6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scrapy/project_faces/spiders/FindFaces.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "class FindfacesSpider(scrapy.Spider):\n",
        "    name = 'FindFaces'\n",
        "    allowed_domains = ['www.investigart.com']\n",
        "    start_urls = ['https://www.investigart.com/']\n",
        "    pages = 1\n",
        "\n",
        "    def parse(self, response):\n",
        "      posts = response.css('.et_pb_salvattore_content > .et_pb_post')\n",
        "      print('numero de entradas:', len(posts))\n",
        "      for post in posts:\n",
        "        href = post.css('.et_pb_image_container > a::attr(href)').get()\n",
        "        print('href', href)\n",
        "        yield scrapy.Request(href, callback = self.parse_post, meta={'href': href})\n",
        "      next_page = response.css('.nextpostslink')\n",
        "      self.pages += 1\n",
        "      if next_page and self.pages < 4:\n",
        "        next_href = next_page.css('a::attr(href)').get()\n",
        "        yield scrapy.Request(next_href)\n",
        "\n",
        "    def parse_post(self, response):\n",
        "      href = response.meta.get('href')\n",
        "      element = response.xpath('//*[contains(@class, \"post-meta\")]/following-sibling::img')\n",
        "      img = element.css('img::attr(src)').get()\n",
        "      yield {\n",
        "        'href': href,\n",
        "        'img': img\n",
        "      }"
      ],
      "metadata": {
        "id": "RwIV7tTolQfD",
        "outputId": "452a1ad7-952b-4d0f-ebb2-0746566d4b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scrapy/project_faces/spiders/FindFaces.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesando las caras de las imágenes"
      ],
      "metadata": {
        "id": "Ax7-CaQsnZDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import json\n",
        "import numpy as np\n",
        "import requests"
      ],
      "metadata": {
        "id": "mBgJljM0x0rN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el archivo jason en una variable\n",
        "with open('scrapy/project_faces/faces.json', 'r') as f:\n",
        "  json_data = json.loads(f.read())\n",
        "\n",
        "# Sacamos las url de las fotos\n",
        "for i in range(0,15):\n",
        "  my_variable = json_data[1]"
      ],
      "metadata": {
        "id": "WPzspLK2-mXn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_variable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruE5dQEmddYY",
        "outputId": "650c0351-b98a-438e-84f9-cf0e7a274c72"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'href': 'https://www.investigart.com/2022/12/07/orgullobarroco-en-un-lugar-de-la-macha-herencia/', 'img': 'https://www.investigart.com/wp-content/uploads/2022/12/318407191_512829410878356_2455083222164072723_n.jpg'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nos descargaremos las fotos en nuestro entorno\n",
        "\n",
        "!wget \"https://www.investigart.com/wp-content/uploads/2022/10/Captura-de-pantalla-92.png\"\n",
        "ulr=\"https://www.investigart.com/wp-content/uploads/2022/10/Captura-de-pantalla-92.png\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxW6LUIqwp09",
        "outputId": "54f42d69-4d40-4c53-d153-ab9cf944dc39"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-15 16:46:45--  https://www.investigart.com/wp-content/uploads/2022/10/Captura-de-pantalla-92.png\n",
            "Resolving www.investigart.com (www.investigart.com)... 37.32.98.14\n",
            "Connecting to www.investigart.com (www.investigart.com)|37.32.98.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 292682 (286K) [image/png]\n",
            "Saving to: ‘Captura-de-pantalla-92.png.1’\n",
            "\n",
            "Captura-de-pantalla 100%[===================>] 285.82K   353KB/s    in 0.8s    \n",
            "\n",
            "2022-12-15 16:46:47 (353 KB/s) - ‘Captura-de-pantalla-92.png.1’ saved [292682/292682]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos la foto\n",
        "image = cv2.imread('image2.jpg')\n",
        "\n",
        "# Aplicaciones el clasificador, el cuál se descargará en https://github.com/kipr/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml \n",
        "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Detectamos las caras que hay en la foto\n",
        "faces = classifier.detectMultiScale(image)\n",
        "\n",
        "# Enseñamos por pantalla el numero de caras que se han detectado\n",
        "print(f'{len(faces)} faces were detected.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yCTGodKx_0O",
        "outputId": "372322d3-665e-4c36-b1c6-af9d3fce8197"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 faces were detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "arrayfotos=[]\n",
        "\n",
        "image = cv2.imread('image2.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Como antes detectamos las caras\n",
        "faces = classifier.detectMultiScale(image)\n",
        "\n",
        "# Dibujamos un cuadrado alrededor de las caras detectadas\n",
        "for (x, y, w, h) in faces:\n",
        "  cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "# Si queremos mostrar por pantalla la foto con las caras detectadas\n",
        "\"\"\"cv2_imshow(image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\"\"\"\n",
        "\n",
        "i=0\n",
        "\n",
        "# Recortar las caras detectadas\n",
        "for (x,y,w,h) in faces:\n",
        "  face = image[y:y+h, x:x+w]\n",
        "\n",
        "#Guardar por separado los recortes de las caras\n",
        "  cv2.imwrite('face_{}.jpg'.format(i), face)\n",
        "  arrayfotos.append('face_{}.jpg'.format(i))\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "tQwYdgDDgipS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(arrayfotos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiKUfIaZ2ZuY",
        "outputId": "dfc67220-05a4-4d80-da58-38a9f2f420db"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['face_0.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img in arrayfotos:\n",
        "  image = Image.open(img)\n",
        "\n",
        "  # Reescalamos las fotos para que todas sean de 200x200\n",
        "  image = image.resize((200, 200), Image.ANTIALIAS)\n",
        "\n",
        "  # Guardamos las imagenes reescaladas\n",
        "  image.save(img)"
      ],
      "metadata": {
        "id": "9xdFCCVW6drG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una imagen en blanco de dimensiones que queramos\n",
        "width = 800\n",
        "height = 600\n",
        "collage = Image.new('RGB', (width, height), (255,255,255))\n",
        "\n",
        "# Cargamos los recortes de las caras\n",
        "photo1 = Image.open('face_0.jpg')\n",
        "\n",
        "# Pegamos las fotos en la imagen en blanco \n",
        "collage.paste(photo1, (0, 0))\n",
        "\n",
        "# Guardamos el resultado en el collage\n",
        "collage.save('collage.jpg')"
      ],
      "metadata": {
        "id": "1E93EiQ677jH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HAY QUE MEJORARLO BASTANTE CUANDO PUEDA!"
      ],
      "metadata": {
        "id": "Esn-pW5BdKRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}